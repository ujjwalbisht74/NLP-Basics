{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Preprocessing Technique for NLP\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Lowercasing in NLP is the process of converting all uppercase letters in a text to lowercase. It's done to make the text consistent and easier to work with. By reducing the complexity of the text, lowercasing helps in analyzing and processing it more effectively.\n",
    "\n",
    "\n",
    "## Improtance of Lowercasing\n",
    "\n",
    "- 1>`Text Normalization` : By converting text to lowercase, different cases of the same word are treated as a single entity. This normalization step helps simplify the analysis by reducing the number of distinct word forms. \n",
    "\n",
    "\"For example, \"Cat\" and \"cat\" both refer to the same concept (but taken as different due to case sensitive property of languages), and lowercasing them allows treating them as identical.\"\n",
    "\n",
    "\n",
    "- 2`Vocabulary Reduction` ->   Lowercasing reduces the vocabulary size by treating the same word in different cases as a single term. This simplifies tasks such as text classification, sentiment analysis, or information retrieval, where treating different cases as distinct entities may not be desired.\n",
    "\n",
    "- 3`Consistency` -> Lowercasing ensures consistent representation of words. Maintaining a consistent capitalization style across the text makes it easier to process and analyze. \n",
    "\n",
    "\"For example, \"New York\" and \"new york\" could refer to the same entity, but lowercasing makes them consistent and treatable as a single entity.\"\n",
    "\n",
    "- 4`Case Insenstivity` ->  Lowercasing makes the text case-insensitive, which can be beneficial in scenarios where the case information is not relevant for the analysis or processing tasks. It simplifies matching or comparing words, regardless of their original case\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## USAGE\n",
    "\n",
    "Following are the defined and generalized steps to apply lowercasing in NLP pipeline :\n",
    "\n",
    "-`Tokenization : ` Break down of text into tokens or indivisual word.\n",
    "\n",
    "-`Lowercase Coversion : ` Apply lowercase transformation to each token, converting all uppercase letters to lowercase.\n",
    "\n",
    "#### NOTE -> Code implementations are provided in respective notebook in both R and Python programming language. \n",
    "\n",
    "\n",
    "\n",
    "## Considerations\n",
    "\n",
    "- `While lowercasing is a commonly used preprocessing step in NLP, it's important to consider its appropriateness for specific tasks. Here are some key factors to keep in mind : `\n",
    "\n",
    "- `Context : ` In certain NLP tasks like named entity recognition or sentiment analysis, the case of words can carry valuable information. Lowercasing in such cases may lead to the loss of important nuances. \n",
    "\n",
    "For example, distinguishing between \"Apple\" (the company) and \"apple\" (the fruit) relies on the case\n",
    "\n",
    "- `Task-specific Requirements : ` Depending on your particular NLP task, you may need to preserve the case information. Consider the objectives and requirements of your project. \n",
    "\n",
    "For instance, if you're working on a task that involves detecting proper nouns or acronyms, lowercasing might not be suitable.\n",
    "\n",
    "- `Language Considerations : ` Different languages have varying conventions for case sensitivity. Some languages, like English, are typically case-insensitive, while others, like German, have specific case rules. It's crucial to consider the linguistic characteristics of the language you're working with to determine whether lowercasing is appropriate or desirable.\n",
    "\n",
    "-`Named Entities : ` Named entities, such as person names, organization names, or locations, often have specific capitalization patterns. Lowercasing these entities might lead to ambiguity or loss of important information. Determine if your task involves identifying or analyzing named entities.\n",
    "\n",
    "- `Acronyms and Abbreviations : ` Acronyms and abbreviations often rely on capitalization for recognition and understanding. Lowercasing them could result in the loss of their intended meaning. Check if your task involves handling acronyms or abbreviations.\n",
    "\n",
    "- `Sentiment Analysis : ` In sentiment analysis or opinion mining, the capitalization of words can sometimes convey emphasis or sentiment. Lowercasing might flatten or alter the sentiment expressed in the text. Consider if sentiment analysis is a part of your task.\n",
    "\n",
    "- `Language-specific Rules : ` Different languages have distinct rules regarding case sensitivity. Some languages, like English, are largely case-insensitive, while others, like Turkish, have specific case rules. Ensure that you understand the case conventions of the language you're working with.\n",
    "\n",
    "- `Domain-specific Considerations : ` Certain domains or industries might have their own conventions for capitalization. For example, legal documents or scientific literature may follow specific capitalization styles. Take into account any domain-specific requirements for your task.\n",
    "\n",
    "- `Task-specific Objectives : ` Consider the objectives of your NLP task and whether lowercasing aligns with them. Determine if lowercasing improves or hinders the performance of subsequent analysis steps, such as classification, clustering, or information retrieval.\n",
    "\n",
    "- `Preserving Original Formatting : ` In some cases, it might be important to preserve the original formatting of the text, including capitalization. This is relevant when maintaining the original appearance or style is necessary, such as in text generation or dialogue systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Basic PreProcessing\\Corona_NLP_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>44953</td>\n",
       "      <td>NYC</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44954</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>44955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>44956</td>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>44957</td>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName             Location     TweetAt  \\\n",
       "0         1       44953                  NYC  02-03-2020   \n",
       "1         2       44954          Seattle, WA  02-03-2020   \n",
       "2         3       44955                  NaN  02-03-2020   \n",
       "3         4       44956          Chicagoland  02-03-2020   \n",
       "4         5       44957  Melbourne, Victoria  03-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  TRENDING: New Yorkers encounter empty supermar...  Extremely Negative  \n",
       "1  When I couldn't find hand sanitizer at Fred Me...            Positive  \n",
       "2  Find out how you can protect yourself and love...  Extremely Positive  \n",
       "3  #Panic buying hits #NewYork City as anxious sh...            Negative  \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Panic buying hits #NewYork City as anxious shoppers stock up on food&amp;medical supplies after #healthcare worker in her 30s becomes #BigApple 1st confirmed #coronavirus patient OR a #Bloomberg staged event?\n",
      "\n",
      "https://t.co/IASiReGPC4\n",
      "\n",
      "#QAnon #QAnon2018 #QAnon2020 \n",
      "#Election2020 #CDC https://t.co/29isZOewxu\n"
     ]
    }
   ],
   "source": [
    "# Extracting a peice of  statement and understand how to perform lowercasing\n",
    " \n",
    "sample_text = df[\"OriginalTweet\"][3]\n",
    "\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#panic buying hits #newyork city as anxious shoppers stock up on food&amp;medical supplies after #healthcare worker in her 30s becomes #bigapple 1st confirmed #coronavirus patient or a #bloomberg staged event?\n",
      "\n",
      "https://t.co/iasiregpc4\n",
      "\n",
      "#qanon #qanon2018 #qanon2020 \n",
      "#election2020 #cdc https://t.co/29iszoewxu\n"
     ]
    }
   ],
   "source": [
    "lower_text = sample_text.lower()\n",
    "\n",
    "# So why we lowering it ??\n",
    "\n",
    "# Whenever we works with python (as we know as it is case sensitive ), so it will take same word but with different format as two indivisuals which increase complexity of model to prevent it we perform lowercasing. \n",
    "\n",
    "print(lower_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       trending: new yorkers encounter empty supermar...\n",
       "1       when i couldn't find hand sanitizer at fred me...\n",
       "2       find out how you can protect yourself and love...\n",
       "3       #panic buying hits #newyork city as anxious sh...\n",
       "4       #toiletpaper #dunnypaper #coronavirus #coronav...\n",
       "                              ...                        \n",
       "3793    meanwhile in a supermarket in israel -- people...\n",
       "3794    did you panic buy a lot of non-perishable item...\n",
       "3795    asst prof of economics @cconces was on @nbcphi...\n",
       "3796    gov need to do somethings instead of biar je r...\n",
       "3797    i and @forestandpaper members are committed to...\n",
       "Name: OriginalTweet, Length: 3798, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing lowercase to complete coloumns\n",
    "\n",
    "\n",
    "updated_text= df[\"OriginalTweet\"].str.lower()\n",
    "\n",
    "\n",
    "# So it is easy ....\n",
    "updated_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMOVING REDUDENCIES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample tet with html tags ...\n",
    "text = \"<p>Lorem ipsum dolor sit, amet consectetur adipisicing elit. Veritatis quae cum totam! Adipisci fuga dolor inventore labore voluptate alias, consequuntur, corrupti ea non iste autem quo quam dignissimos, quasi repellat. Sit molestias aut temporibus voluptatum quae. Incidunt fugit nisi eum, quae similique provident atque accusamus. Est, aperiam ipsum, placeat corporis aliquam inventore sunt autem accusantium, nobis fugit id rem laboriosam quae asperiores nulla enim incidunt et dolorum vel quam cumque ad! Quisquam, natus odio? Distinctio, consectetur maiores. Ad, beatae qui dolorem itaque culpa odit vero accusamus quo quam voluptas, impedit soluta dolorum facilis laboriosam eum? Eum amet nemo, repellat qui deleniti obcaecati placeat totam molestias assumenda sit vitae reprehenderit perspiciatis porro, asperiores quaerat fugiat ratione. Blanditiis nobis laboriosam quae fugiat culpa et, odit quaerat, commodi consectetur expedita inventore eos, provident deserunt! Sunt doloremque, blanditiis veniam velit culpa pariatur eveniet error sit a molestias voluptates deleniti obcaecati hic corporis aut maxime. Voluptates laudantium obcaecati molestias quidem voluptatum! Illo deleniti beatae magnam, provident ullam eveniet reprehenderit et! Aliquid illum consequuntur incidunt quis saepe atque nulla, molestiae eveniet quia deleniti at excepturi nisi sunt natus perspiciatis tempore dignissimos similique tenetur nesciunt vero culpa! Quasi, repudiandae facere nemo obcaecati soluta unde similique suscipit, quae tempore veniam minima eum distinctio illum harum temporibus, debitis aspernatur sed mollitia. Nesciunt veniam itaque delectus praesentium iste! Saepe consequuntur, sapiente numquam nobis molestiae totam perspiciatis. Id sapiente ad cupiditate libero inventore totam blanditiis, necessitatibus quis rerum, dolore minus nam debitis cum accusamus dolorem sit ex vel aut consequuntur et enim ipsa exercitationem iure! Quidem expedita illum, nisi accusamus unde, repudiandae exercitationem accusantium et iste mollitia amet voluptatem dolore labore omnis nesciunt facilis autem facere voluptate tenetur quod cumque alias qui! Fugiat vero tempora velit ipsum dicta ipsa, ullam eius eum earum labore. Voluptate omnis optio autem ut neque minima labore alias quam excepturi officiis est laborum nam, officia aperiam rem consequuntur nemo cupiditate reprehenderit dolorum a facere modi quod odio! Omnis quidem a minus provident nam, ipsa reiciendis rerum tempora est, quasi ipsam maxime esse maiores sint perferendis tenetur eum ad sed, ipsum corrupti earum! Magnam, tenetur? Magni asperiores mollitia inventore incidunt recusandae suscipit odit a voluptatum cum. Aspernatur sapiente minus, aperiam doloribus possimus dolorem magni. Quam exercitationem voluptatum molestias sapiente provident odit excepturi at, omnis laborum natus laboriosam tempora cupiditate fugit quia voluptates quod aperiam pariatur eveniet esse dignissimos id enim ducimus animi debitis? Enim aliquam nihil id laboriosam ea autem earum ratione quo corrupti suscipit nesciunt vel omnis corporis quis, distinctio voluptatibus unde? Aperiam necessitatibus quae suscipit earum quo ad, nemo beatae. Facilis in adipisci ad ducimus temporibus possimus eum, exercitationem nam cupiditate autem architecto, accusantium nobis minus aperiam, voluptatem enim neque dolore! Dolorum non unde iusto laborum tenetur explicabo. At quidem laborum nulla? Architecto facilis molestiae accusamus cumque iure voluptates vitae recusandae repellat corrupti reiciendis, debitis a placeat doloremque ipsam quod repudiandae pariatur vel quisquam atque corporis autem est iusto adipisci dolore! Explicabo, voluptatem sed. Perferendis nostrum optio itaque ut. Mollitia non voluptate velit repudiandae nulla laudantium eligendi consectetur quia ratione cumque, totam tenetur ex dolore pariatur blanditiis laboriosam impedit officia modi culpa, necessitatibus nisi. Molestias omnis sunt numquam dicta natus maiores assumenda. Odit voluptatum, doloremque accusamus eaque labore cum! Omnis consequatur quo, maiores error praesentium quos necessitatibus natus odit distinctio harum architecto reprehenderit molestiae! Incidunt vero nulla fugit nisi itaque reprehenderit at natus animi! Alias provident sapiente hic laborum aliquam. Ea libero provident exercitationem possimus, autem ipsum aperiam velit reprehenderit, dolor tenetur eveniet optio quo quas atque magnam pariatur, accusamus beatae placeat! Saepe, nobis corrupti, nostrum delectus commodi itaque numquam ab ullam doloribus vel error eveniet, labore quia id! Impedit placeat cumque quod vero alias ipsam aspernatur hic vitae! Molestiae, repellendus deserunt repudiandae esse beatae impedit qui accusamus rerum et reprehenderit accusantium, repellat ducimus similique autem sequi reiciendis magnam expedita, veniam mollitia maiores in! Dolores quibusdam at iusto, ex inventore suscipit eligendi voluptates, ipsam ducimus saepe, odio commodi eos nostrum recusandae in ad eum quas ut amet adipisci molestias! Sequi eius voluptatum corporis quis nobis adipisci architecto aspernatur aliquam necessitatibus perferendis et, laborum explicabo unde error provident? Qui culpa dolorem, animi omnis saepe consectetur dolor deleniti natus numquam, sequi aspernatur delectus voluptatem, eum similique? Rem tempore repellat assumenda quam amet quidem molestias in autem doloribus error? Rerum ullam dolores omnis aliquam, culpa, quaerat perspiciatis obcaecati iure, accusantium corrupti recusandae. Quibusdam magnam voluptatibus numquam omnis? Ratione a quisquam blanditiis eligendi iste? Possimus repellat nesciunt pariatur accusantium placeat officiis soluta dolore. Laborum optio dolores ad quasi molestias deserunt odio, unde libero obcaecati numquam at sequi magni voluptatem dolorum, corporis repellat, reprehenderit architecto. Accusantium perspiciatis, beatae laboriosam aliquid molestias provident cum. Dolor, consectetur vel provident quam et dolores sequi ipsa suscipit adipisci sed enim incidunt porro sapiente, placeat odit corrupti possimus hic ipsam earum veritatis, illo harum! Similique veritatis sequi beatae, nam accusantium numquam minus vitae atque? Amet sapiente veritatis vero error accusantium alias rem, a esse id aliquid enim. Suscipit nesciunt quia aliquid ipsa officiis eum molestias, ad distinctio magni dolorum? Velit nulla laboriosam, voluptate vitae doloremque ipsam animi obcaecati libero temporibus unde qui voluptas dolores odio impedit. Numquam sunt doloribus nobis necessitatibus molestiae eveniet eligendi, hic temporibus laboriosam repellendus, libero maxime quos minima corporis. Eaque saepe, quibusdam laboriosam expedita voluptates reiciendis distinctio perspiciatis repellendus minus praesentium nobis magni est tenetur itaque provident dolorem cumque dicta tempore facere quam! Atque dignissimos nemo nobis consequatur, pariatur ipsa molestias eaque aliquid nihil voluptates culpa autem asperiores rerum aut. Dolore at voluptatum dolorum, consequatur cumque tempora libero neque, architecto molestias incidunt facere, eos minus ipsum hic tempore vel nihil quisquam! Quidem quaerat ea suscipit aliquid molestias in, quibusdam amet perspiciatis vel impedit quod similique earum recusandae pariatur dignissimos quos laboriosam hic, deserunt consequuntur? Quas veritatis recusandae animi ipsam consequatur aliquid a magnam fugiat. Harum pariatur et, rem non, quod accusantium accusamus aut aliquid, in voluptatibus eligendi! Fugiat quam harum cum, voluptates ex possimus explicabo quaerat officia corporis accusantium, ipsam suscipit? Vero dicta cum sed, nesciunt exercitationem illum voluptate ipsam ex!<p><h1>Welcome to tutorial</h1><p>Lorem ipsum, dolor sit amet consectetur adipisicing elit. Repellat voluptatem quam cum distinctio molestias quia quisquam, dignissimos eos consequuntur vero veniam animi reprehenderit architecto omnis iusto numquam perferendis ratione inventore.</p></p></p>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function using regular expression to remove html tags\n",
    "import re \n",
    "\n",
    "def removeTags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lorem ipsum dolor sit, amet consectetur adipisicing elit. Veritatis quae cum totam! Adipisci fuga dolor inventore labore voluptate alias, consequuntur, corrupti ea non iste autem quo quam dignissimos, quasi repellat. Sit molestias aut temporibus voluptatum quae. Incidunt fugit nisi eum, quae similique provident atque accusamus. Est, aperiam ipsum, placeat corporis aliquam inventore sunt autem accusantium, nobis fugit id rem laboriosam quae asperiores nulla enim incidunt et dolorum vel quam cumque ad! Quisquam, natus odio? Distinctio, consectetur maiores. Ad, beatae qui dolorem itaque culpa odit vero accusamus quo quam voluptas, impedit soluta dolorum facilis laboriosam eum? Eum amet nemo, repellat qui deleniti obcaecati placeat totam molestias assumenda sit vitae reprehenderit perspiciatis porro, asperiores quaerat fugiat ratione. Blanditiis nobis laboriosam quae fugiat culpa et, odit quaerat, commodi consectetur expedita inventore eos, provident deserunt! Sunt doloremque, blanditiis veniam velit culpa pariatur eveniet error sit a molestias voluptates deleniti obcaecati hic corporis aut maxime. Voluptates laudantium obcaecati molestias quidem voluptatum! Illo deleniti beatae magnam, provident ullam eveniet reprehenderit et! Aliquid illum consequuntur incidunt quis saepe atque nulla, molestiae eveniet quia deleniti at excepturi nisi sunt natus perspiciatis tempore dignissimos similique tenetur nesciunt vero culpa! Quasi, repudiandae facere nemo obcaecati soluta unde similique suscipit, quae tempore veniam minima eum distinctio illum harum temporibus, debitis aspernatur sed mollitia. Nesciunt veniam itaque delectus praesentium iste! Saepe consequuntur, sapiente numquam nobis molestiae totam perspiciatis. Id sapiente ad cupiditate libero inventore totam blanditiis, necessitatibus quis rerum, dolore minus nam debitis cum accusamus dolorem sit ex vel aut consequuntur et enim ipsa exercitationem iure! Quidem expedita illum, nisi accusamus unde, repudiandae exercitationem accusantium et iste mollitia amet voluptatem dolore labore omnis nesciunt facilis autem facere voluptate tenetur quod cumque alias qui! Fugiat vero tempora velit ipsum dicta ipsa, ullam eius eum earum labore. Voluptate omnis optio autem ut neque minima labore alias quam excepturi officiis est laborum nam, officia aperiam rem consequuntur nemo cupiditate reprehenderit dolorum a facere modi quod odio! Omnis quidem a minus provident nam, ipsa reiciendis rerum tempora est, quasi ipsam maxime esse maiores sint perferendis tenetur eum ad sed, ipsum corrupti earum! Magnam, tenetur? Magni asperiores mollitia inventore incidunt recusandae suscipit odit a voluptatum cum. Aspernatur sapiente minus, aperiam doloribus possimus dolorem magni. Quam exercitationem voluptatum molestias sapiente provident odit excepturi at, omnis laborum natus laboriosam tempora cupiditate fugit quia voluptates quod aperiam pariatur eveniet esse dignissimos id enim ducimus animi debitis? Enim aliquam nihil id laboriosam ea autem earum ratione quo corrupti suscipit nesciunt vel omnis corporis quis, distinctio voluptatibus unde? Aperiam necessitatibus quae suscipit earum quo ad, nemo beatae. Facilis in adipisci ad ducimus temporibus possimus eum, exercitationem nam cupiditate autem architecto, accusantium nobis minus aperiam, voluptatem enim neque dolore! Dolorum non unde iusto laborum tenetur explicabo. At quidem laborum nulla? Architecto facilis molestiae accusamus cumque iure voluptates vitae recusandae repellat corrupti reiciendis, debitis a placeat doloremque ipsam quod repudiandae pariatur vel quisquam atque corporis autem est iusto adipisci dolore! Explicabo, voluptatem sed. Perferendis nostrum optio itaque ut. Mollitia non voluptate velit repudiandae nulla laudantium eligendi consectetur quia ratione cumque, totam tenetur ex dolore pariatur blanditiis laboriosam impedit officia modi culpa, necessitatibus nisi. Molestias omnis sunt numquam dicta natus maiores assumenda. Odit voluptatum, doloremque accusamus eaque labore cum! Omnis consequatur quo, maiores error praesentium quos necessitatibus natus odit distinctio harum architecto reprehenderit molestiae! Incidunt vero nulla fugit nisi itaque reprehenderit at natus animi! Alias provident sapiente hic laborum aliquam. Ea libero provident exercitationem possimus, autem ipsum aperiam velit reprehenderit, dolor tenetur eveniet optio quo quas atque magnam pariatur, accusamus beatae placeat! Saepe, nobis corrupti, nostrum delectus commodi itaque numquam ab ullam doloribus vel error eveniet, labore quia id! Impedit placeat cumque quod vero alias ipsam aspernatur hic vitae! Molestiae, repellendus deserunt repudiandae esse beatae impedit qui accusamus rerum et reprehenderit accusantium, repellat ducimus similique autem sequi reiciendis magnam expedita, veniam mollitia maiores in! Dolores quibusdam at iusto, ex inventore suscipit eligendi voluptates, ipsam ducimus saepe, odio commodi eos nostrum recusandae in ad eum quas ut amet adipisci molestias! Sequi eius voluptatum corporis quis nobis adipisci architecto aspernatur aliquam necessitatibus perferendis et, laborum explicabo unde error provident? Qui culpa dolorem, animi omnis saepe consectetur dolor deleniti natus numquam, sequi aspernatur delectus voluptatem, eum similique? Rem tempore repellat assumenda quam amet quidem molestias in autem doloribus error? Rerum ullam dolores omnis aliquam, culpa, quaerat perspiciatis obcaecati iure, accusantium corrupti recusandae. Quibusdam magnam voluptatibus numquam omnis? Ratione a quisquam blanditiis eligendi iste? Possimus repellat nesciunt pariatur accusantium placeat officiis soluta dolore. Laborum optio dolores ad quasi molestias deserunt odio, unde libero obcaecati numquam at sequi magni voluptatem dolorum, corporis repellat, reprehenderit architecto. Accusantium perspiciatis, beatae laboriosam aliquid molestias provident cum. Dolor, consectetur vel provident quam et dolores sequi ipsa suscipit adipisci sed enim incidunt porro sapiente, placeat odit corrupti possimus hic ipsam earum veritatis, illo harum! Similique veritatis sequi beatae, nam accusantium numquam minus vitae atque? Amet sapiente veritatis vero error accusantium alias rem, a esse id aliquid enim. Suscipit nesciunt quia aliquid ipsa officiis eum molestias, ad distinctio magni dolorum? Velit nulla laboriosam, voluptate vitae doloremque ipsam animi obcaecati libero temporibus unde qui voluptas dolores odio impedit. Numquam sunt doloribus nobis necessitatibus molestiae eveniet eligendi, hic temporibus laboriosam repellendus, libero maxime quos minima corporis. Eaque saepe, quibusdam laboriosam expedita voluptates reiciendis distinctio perspiciatis repellendus minus praesentium nobis magni est tenetur itaque provident dolorem cumque dicta tempore facere quam! Atque dignissimos nemo nobis consequatur, pariatur ipsa molestias eaque aliquid nihil voluptates culpa autem asperiores rerum aut. Dolore at voluptatum dolorum, consequatur cumque tempora libero neque, architecto molestias incidunt facere, eos minus ipsum hic tempore vel nihil quisquam! Quidem quaerat ea suscipit aliquid molestias in, quibusdam amet perspiciatis vel impedit quod similique earum recusandae pariatur dignissimos quos laboriosam hic, deserunt consequuntur? Quas veritatis recusandae animi ipsam consequatur aliquid a magnam fugiat. Harum pariatur et, rem non, quod accusantium accusamus aut aliquid, in voluptatibus eligendi! Fugiat quam harum cum, voluptates ex possimus explicabo quaerat officia corporis accusantium, ipsam suscipit? Vero dicta cum sed, nesciunt exercitationem illum voluptate ipsam ex!Welcome to tutorialLorem ipsum, dolor sit amet consectetur adipisicing elit. Repellat voluptatem quam cum distinctio molestias quia quisquam, dignissimos eos consequuntur vero veniam animi reprehenderit architecto omnis iusto numquam perferendis ratione inventore.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finalizing result\n",
    "\n",
    "removeTags(text) # removed all html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       TRENDING: New Yorkers encounter empty supermar...\n",
       "1       When I couldn't find hand sanitizer at Fred Me...\n",
       "2       Find out how you can protect yourself and love...\n",
       "3       #Panic buying hits #NewYork City as anxious sh...\n",
       "4       #toiletpaper #dunnypaper #coronavirus #coronav...\n",
       "                              ...                        \n",
       "3793    Meanwhile In A Supermarket in Israel -- People...\n",
       "3794    Did you panic buy a lot of non-perishable item...\n",
       "3795    Asst Prof of Economics @cconces was on @NBCPhi...\n",
       "3796    Gov need to do somethings instead of biar je r...\n",
       "3797    I and @ForestandPaper members are committed to...\n",
       "Name: OriginalTweet, Length: 3798, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying tag removal to complete colomn.\n",
    "\n",
    "update_text = df[\"OriginalTweet\"].apply(removeTags)\n",
    "\n",
    "update_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack of Purell??!!Check out how  #coronavirus concerns are driving up prices. https://t.co/ygbipBflMY\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df[\"OriginalTweet\"][1]\n",
    "text # It will contain link which is also a redundancy for model , so its better to remove it \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URLS(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack of Purell??!!Check out how  #coronavirus concerns are driving up prices. \""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check the function for sample text\n",
    "remove_URLS(text) # Its working.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       TRENDING: New Yorkers encounter empty supermar...\n",
       "1       When I couldn't find hand sanitizer at Fred Me...\n",
       "2       Find out how you can protect yourself and love...\n",
       "3       #Panic buying hits #NewYork City as anxious sh...\n",
       "4       #toiletpaper #dunnypaper #coronavirus #coronav...\n",
       "                              ...                        \n",
       "3793    Meanwhile In A Supermarket in Israel -- People...\n",
       "3794    Did you panic buy a lot of non-perishable item...\n",
       "3795    Asst Prof of Economics @cconces was on @NBCPhi...\n",
       "3796    Gov need to do somethings instead of biar je r...\n",
       "3797    I and @ForestandPaper members are committed to...\n",
       "Name: OriginalTweet, Length: 3798, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code to apply it in complete colomn\n",
    "\n",
    "text_without_URL = df[\"OriginalTweet\"].apply(remove_URLS)\n",
    "\n",
    "text_without_URL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of Punctuations by python..\n",
    "\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to remove punctuations\n",
    "\n",
    "def removePunc(text):\n",
    "    for mark in punctuations:\n",
    "        text = text.replace(mark,'')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack of Purell??!!Check out how  #coronavirus concerns are driving up prices. https://t.co/ygbipBflMY\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text = df[\"OriginalTweet\"][19]\n",
    "new_text\n",
    "\n",
    "\n",
    "new_text = removePunc(new_text)\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advance function to remove punctuation\n",
    "\n",
    "def removePunc2(text):\n",
    "    return text.translate(str.maketrans('','',punctuations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Studies show the coronavirus like COVID19 can live up to nine days on hard surfaces like metal plastic and glass\\r\\r\\n\\r\\r\\nOur Deputy Commissioner of Consumer Affairs Mary Barzee Flores shows you how to keep clean at the gas pump\\r\\r\\n\\r\\r\\nWatch and share httpstcoAIqATWT5zz'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removePunc2(new_text) # This is one is more effective "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       TRENDING New Yorkers encounter empty supermark...\n",
       "1       When I couldnt find hand sanitizer at Fred Mey...\n",
       "2       Find out how you can protect yourself and love...\n",
       "3       Panic buying hits NewYork City as anxious shop...\n",
       "4       toiletpaper dunnypaper coronavirus coronavirus...\n",
       "                              ...                        \n",
       "3793    Meanwhile In A Supermarket in Israel  People d...\n",
       "3794    Did you panic buy a lot of nonperishable items...\n",
       "3795    Asst Prof of Economics cconces was on NBCPhila...\n",
       "3796    Gov need to do somethings instead of biar je r...\n",
       "3797    I and ForestandPaper members are committed to ...\n",
       "Name: OriginalTweet, Length: 3798, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying it in datatset\n",
    "\n",
    "df[\"OriginalTweet\"].apply(removePunc2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Word Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we have some words mapping for fast paced word, just an example to ellaborate how chat word treatment works\n",
    "\n",
    "# Mapped slangs with their root words \n",
    "chat_slang = {\n",
    "    \"GN\": \"Good Night\",\n",
    "    \"BTW\" : \"By The Way\",\n",
    "    \"GM\" : \"Good Morning\",\n",
    "    \"U\" : \"You\",\n",
    "    \"U2\" : \"You2\",\n",
    "    \"THY\" : \"Thank You\",\n",
    "    \"TH\" : \"Thanks\",\n",
    "    \"MUS\" : \"Meet You Soon\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word.upper() in chat_slang:\n",
    "            new_text.append(chat_slang[word.upper()])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Hey Rohan GM , TH BTW for tommorow party , MUS \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey Rohan Good Morning , Thanks By The Way for tommorow party , Meet You Soon'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversion(message) # so we had done our deconding ... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "\n",
    "text =  \"Hello,  Have u seen a peice of paper on table this evening. I think I had placed the papr somewhere \"\n",
    "\n",
    "# There are identical words (paper) but in different spelling (paper, papr) which can be a redundancy for model as it increases the complexity    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries for spelling corection\n",
    "\n",
    "from textblob import TextBlob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello,  Have u seen a peace of paper on table this evening. I think I had placed the paper somewhere '"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Istantiate textblob object with input as parameter\n",
    "text_blob_object = TextBlob(text)\n",
    "\n",
    "# check for spelling\n",
    "\n",
    "text_blob_object.correct().string # corrected string\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Panic buying hits #NewYork City as anxious shoppers stock up on food&amp;medical supplies after #healthcare worker in her 30s becomes #BigApple 1st confirmed #coronavirus patient OR a #Bloomberg staged event?\\r\\r\\n\\r\\r\\nhttps://t.co/IASiReGPC4\\r\\r\\n\\r\\r\\n#QAnon #QAnon2018 #QAnon2020 \\r\\r\\n#Election2020 #CDC https://t.co/29isZOewxu'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A random text\n",
    "text = df[\"OriginalTweet\"][3]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function\n",
    "def Remove_stpwrd(text):\n",
    "    new_text= []\n",
    "\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Panic buying hits #NewYork City as anxious shoppers stock up on food&amp;medical supplies after #healthcare worker in her 30s becomes #BigApple 1st confirmed #coronavirus patient OR a #Bloomberg staged event?\\r\\r\\n\\r\\r\\nhttps://t.co/IASiReGPC4\\r\\r\\n\\r\\r\\n#QAnon #QAnon2018 #QAnon2020 \\r\\r\\n#Election2020 #CDC https://t.co/29isZOewxu'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# old\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Panic buying hits #NewYork City  anxious shoppers stock   food&amp;medical supplies  #healthcare worker   30s becomes #BigApple 1st confirmed #coronavirus patient OR  #Bloomberg staged event? https://t.co/IASiReGPC4 #QAnon #QAnon2018 #QAnon2020 #Election2020 #CDC https://t.co/29isZOewxu'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# updated text\n",
    "Remove_stpwrd(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       TRENDING: New Yorkers encounter empty supermar...\n",
       "1       When I  find hand sanitizer  Fred Meyer, I tur...\n",
       "2          Find     protect   loved ones  #coronavirus. ?\n",
       "3       #Panic buying hits #NewYork City  anxious shop...\n",
       "4       #toiletpaper #dunnypaper #coronavirus #coronav...\n",
       "                              ...                        \n",
       "3793    Meanwhile In A Supermarket  Israel -- People d...\n",
       "3794    Did  panic buy  lot  non-perishable items? ECH...\n",
       "3795    Asst Prof  Economics @cconces   @NBCPhiladelph...\n",
       "3796    Gov need   somethings instead  biar je rakyat ...\n",
       "3797    I  @ForestandPaper members  committed   safety...\n",
       "Name: OriginalTweet, Length: 3798, dtype: object"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying it on dataset\n",
    "\n",
    "df[\"OriginalTweet\"].apply(Remove_stpwrd) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have two options for emojis either remove it or convert it ...\n",
    "\n",
    "# Conversion : \n",
    "\n",
    "import re\n",
    "\n",
    "def emojisTransform(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                                u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
    "                                u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
    "                                u\"\\U0001F600-\\U0001F6FF\" # transport and map symbols\n",
    "                                u\"\\U0001F1E0-\\U0001F1FF\" # flags\n",
    "                                \"]+\", flags =re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loved the movie , It is fantastic \\U0001fae1\\U0001fae1\\U0001fae1'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emojisTransform(\"loved the movie , It is fantastic 😁😁😁😁🫡🫡🫡\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loved the movie , It is fantastic :beaming_face_with_smiling_eyes::beaming_face_with_smiling_eyes::beaming_face_with_smiling_eyes::beaming_face_with_smiling_eyes::saluting_face::saluting_face::saluting_face:'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way to interpret emojis\n",
    "\n",
    "import emoji \n",
    "\n",
    "emoji.demojize(\"loved the movie , It is fantastic 😁😁😁😁🫡🫡🫡\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization (Important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       TRENDING: New Yorkers encounter empty supermar...\n",
       "1       When I couldn't find hand sanitizer at Fred Me...\n",
       "2       Find out how you can protect yourself and love...\n",
       "3       #Panic buying hits #NewYork City as anxious sh...\n",
       "4       #toiletpaper #dunnypaper #coronavirus #coronav...\n",
       "                              ...                        \n",
       "3793    Meanwhile In A Supermarket in Israel -- People...\n",
       "3794    Did you panic buy a lot of non-perishable item...\n",
       "3795    Asst Prof of Economics @cconces was on @NBCPhi...\n",
       "3796    Gov need to do somethings instead of biar je r...\n",
       "3797    I and @ForestandPaper members are committed to...\n",
       "Name: OriginalTweet, Length: 3798, dtype: object"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample corpus\n",
    "corpus = df[\"OriginalTweet\"]\n",
    "corpus\n",
    "\n",
    "# Why tokenization important ??\n",
    "\n",
    "# its overall affect the model if you not able to send the actual required tokens for feature engineering...\n",
    "\n",
    "# Lets approach it from most basic approach to advance one     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Panic',\n",
       " 'buying',\n",
       " 'hits',\n",
       " 'NewYork',\n",
       " 'City',\n",
       " 'as',\n",
       " 'anxious',\n",
       " 'shoppers',\n",
       " 'stock',\n",
       " 'up',\n",
       " 'on',\n",
       " 'foodampmedical',\n",
       " 'supplies',\n",
       " 'after',\n",
       " 'healthcare',\n",
       " 'worker',\n",
       " 'in',\n",
       " 'her',\n",
       " '30s',\n",
       " 'becomes',\n",
       " 'BigApple',\n",
       " '1st',\n",
       " 'confirmed',\n",
       " 'coronavirus',\n",
       " 'patient',\n",
       " 'OR',\n",
       " 'a',\n",
       " 'Bloomberg',\n",
       " 'staged',\n",
       " 'event',\n",
       " 'httpstcoIASiReGPC4',\n",
       " 'QAnon',\n",
       " 'QAnon2018',\n",
       " 'QAnon2020',\n",
       " 'Election2020',\n",
       " 'CDC',\n",
       " 'httpstco29isZOewxu']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using split function which keeps eye on spacing \" \"...\n",
    "\n",
    "sample_text = removePunc(sample_text)\n",
    "\n",
    "sample_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey', 'i', 'am', 'new', 'to', 'new', 'delhi']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see some cases of Tokenization you face\n",
    "\n",
    "sample_case = \"hey i am new to new delhi\"\n",
    "\n",
    "sample_case.split() # here it will takenize new delhi to seperate token (new, delhi) which again leads to redundancy during model creation and affect performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey', 'i', 'am', 'new', 'to', 'new', 'delhi']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using regular expression\n",
    "\n",
    "import re \n",
    "\n",
    "tokens =re.findall('[\\w]+',sample_case)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['welcome', '!', 'we', 'are', 'your', 'new', 'friend', '?']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using NLTK \n",
    "\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "sample_case = \"welcome! we are your new friend ? \"\n",
    "word_tokenize(sample_case)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing different libraries realted to stemming\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Instationa stemmer object\n",
    "stemObj = PorterStemmer()\n",
    "\n",
    "# Define function for stemming using Comprehension\n",
    "def stemWords(text):\n",
    "    return\" \".join([stemObj.stem(word) for word in text.split()]) \n",
    "\n",
    "\n",
    "# test sample\n",
    "\n",
    "sample1 = \"walk walks walking walked\"\n",
    "\n",
    "stemWords(sample1) # done....\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "Running             Running             \n",
      "Swimming            Swimming            \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "in                  in                  \n",
      "coherent            coherent            \n",
      "manner              manner              \n",
      "might               might               \n",
      "harm                harm                \n",
      "your                your                \n",
      "health              health              \n",
      "mentally            mentally            \n",
      "and                 and                 \n",
      "pyhsically          pyhsically          \n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "punctuations = string.punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Instantialte object \n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Sample Case\n",
    "sentence = \"Running ,Swimming and eating in coherent manner might harm your health mentally and pyhsically.\"\n",
    "\n",
    "\n",
    "sent_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "for word in sent_words:\n",
    "    if word in punctuations:\n",
    "        sent_words.remove(word)\n",
    "\n",
    "\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in  sent_words:\n",
    "    print(\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
